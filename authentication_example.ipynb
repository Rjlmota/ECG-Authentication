{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv\n",
    "# path = 'physiobank_tool/ecgiddb/per_person'\n",
    "# output_path = 'physiobank_tool/ecgiddb/per_person_output'\n",
    "# files = []\n",
    "# with open(f'{path}/header.txt') as f:\n",
    "#     reader = csv.reader(f)\n",
    "#     for row in reader:\n",
    "#         files.append(row[0])\n",
    "# # files.pop(-1)\n",
    "# # files.pop(-1)\n",
    "# # files.pop(13)\n",
    "# # files.pop(2)\n",
    "# print(files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features_df = pd.DataFrame()\n",
    "# final_df = pd.DataFrame()\n",
    "\n",
    "# for file in files:    \n",
    "#     df = pd.read_csv(f'{output_path}/{file}.csv')\n",
    "#     df = df.drop(df.columns[0], axis=1)\n",
    "#     #df['person'] = 'person_' + str(file)\n",
    "#     df['person'] = str(file)\n",
    "#     features_df = pd.concat([df, features_df], ignore_index=True)\n",
    "\n",
    "# # drivedb_final1.csv\n",
    "# features_df.to_csv('ecgiddb_final_test.csv', index=False)\n",
    "# #features_df.drop(['qrs_interval.1'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_q</th>\n",
       "      <th>mean_r</th>\n",
       "      <th>mean_s</th>\n",
       "      <th>mean_p</th>\n",
       "      <th>mean_t</th>\n",
       "      <th>stdev_q</th>\n",
       "      <th>stdev_r</th>\n",
       "      <th>stdev_s</th>\n",
       "      <th>mean_rr_interval</th>\n",
       "      <th>mean_rq_amplitude</th>\n",
       "      <th>mean_qrs_interval</th>\n",
       "      <th>mean_qs_distance</th>\n",
       "      <th>mean_qt_distance</th>\n",
       "      <th>mean_qrs_offset</th>\n",
       "      <th>mean_qrs_onset</th>\n",
       "      <th>person</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.758271</td>\n",
       "      <td>0.424880</td>\n",
       "      <td>-0.115156</td>\n",
       "      <td>0.424880</td>\n",
       "      <td>0.424880</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.183152</td>\n",
       "      <td>48.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-0.106525</td>\n",
       "      <td>0.141204</td>\n",
       "      <td>tr13-0638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.469886</td>\n",
       "      <td>0.388483</td>\n",
       "      <td>-0.345001</td>\n",
       "      <td>0.388483</td>\n",
       "      <td>0.391162</td>\n",
       "      <td>0.263695</td>\n",
       "      <td>0.038265</td>\n",
       "      <td>0.199686</td>\n",
       "      <td>511.666667</td>\n",
       "      <td>0.858369</td>\n",
       "      <td>107.5</td>\n",
       "      <td>94.5</td>\n",
       "      <td>41.5</td>\n",
       "      <td>-0.307420</td>\n",
       "      <td>-0.266696</td>\n",
       "      <td>tr13-0638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.706730</td>\n",
       "      <td>0.465590</td>\n",
       "      <td>-0.091165</td>\n",
       "      <td>0.465590</td>\n",
       "      <td>0.465590</td>\n",
       "      <td>0.058935</td>\n",
       "      <td>0.089288</td>\n",
       "      <td>0.014919</td>\n",
       "      <td>1165.000000</td>\n",
       "      <td>1.172320</td>\n",
       "      <td>86.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-0.090837</td>\n",
       "      <td>0.238561</td>\n",
       "      <td>tr13-0638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.737378</td>\n",
       "      <td>0.445347</td>\n",
       "      <td>-0.133931</td>\n",
       "      <td>0.445347</td>\n",
       "      <td>0.445347</td>\n",
       "      <td>0.013977</td>\n",
       "      <td>0.002448</td>\n",
       "      <td>0.002395</td>\n",
       "      <td>1205.000000</td>\n",
       "      <td>1.182725</td>\n",
       "      <td>48.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-0.119522</td>\n",
       "      <td>0.181281</td>\n",
       "      <td>tr13-0638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.697479</td>\n",
       "      <td>0.442879</td>\n",
       "      <td>-0.136184</td>\n",
       "      <td>0.442879</td>\n",
       "      <td>0.442879</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.140359</td>\n",
       "      <td>49.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-0.114810</td>\n",
       "      <td>0.209274</td>\n",
       "      <td>tr13-0638</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean_q    mean_r    mean_s    mean_p    mean_t   stdev_q   stdev_r  \\\n",
       "0 -0.758271  0.424880 -0.115156  0.424880  0.424880  0.000000  0.000000   \n",
       "1 -0.469886  0.388483 -0.345001  0.388483  0.391162  0.263695  0.038265   \n",
       "2 -0.706730  0.465590 -0.091165  0.465590  0.465590  0.058935  0.089288   \n",
       "3 -0.737378  0.445347 -0.133931  0.445347  0.445347  0.013977  0.002448   \n",
       "4 -0.697479  0.442879 -0.136184  0.442879  0.442879  0.000000  0.000000   \n",
       "\n",
       "    stdev_s  mean_rr_interval  mean_rq_amplitude  mean_qrs_interval  \\\n",
       "0  0.000000               NaN           1.183152               48.0   \n",
       "1  0.199686        511.666667           0.858369              107.5   \n",
       "2  0.014919       1165.000000           1.172320               86.0   \n",
       "3  0.002395       1205.000000           1.182725               48.0   \n",
       "4  0.000000               NaN           1.140359               49.0   \n",
       "\n",
       "   mean_qs_distance  mean_qt_distance  mean_qrs_offset  mean_qrs_onset  \\\n",
       "0              35.0              12.0        -0.106525        0.141204   \n",
       "1              94.5              41.5        -0.307420       -0.266696   \n",
       "2              73.0              12.0        -0.090837        0.238561   \n",
       "3              33.0              12.0        -0.119522        0.181281   \n",
       "4              35.0              12.0        -0.114810        0.209274   \n",
       "\n",
       "      person  \n",
       "0  tr13-0638  \n",
       "1  tr13-0638  \n",
       "2  tr13-0638  \n",
       "3  tr13-0638  \n",
       "4  tr13-0638  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_df = pd.read_csv('challange_data/block1000.csv')\n",
    "#features_df.drop(['qrs_interval.1'], axis=1, inplace=True)\n",
    "features_df.head()\n",
    "#print(features_df['person'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features_df = pd.read_csv('drivedb_final_test.csv')\n",
    "# features_df2 = pd.read_csv('mitdb_final_test.csv')\n",
    "# features_df2['person'] = features_df2.person.apply(str)\n",
    "# features_df3 = pd.read_csv(\"ecgiddb_final_test.csv\")\n",
    "\n",
    "# features_df = pd.concat([features_df, features_df2, features_df3], ignore_index=True)\n",
    "\n",
    "# features_df.drop(['qrs_interval.1'], axis=1, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #print(features_df['person'].value_counts())\n",
    "# features_df = features_df[features_df['person'] != 'person234']\n",
    "# features_df.head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contagem de segmentos duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total original:       120000\n",
      "duplicados no original:  5671\n",
      "número de pessoas únicas no original:  1000\n",
      "total sem duplicados:       114329\n",
      "duplicados no final:  0\n",
      "número de pessoas únicas sem dupicados:  1000\n"
     ]
    }
   ],
   "source": [
    "#ecg_id_features.plot(figsize=(20,12))\n",
    "print('total original:      ',len(features_df))\n",
    "print('duplicados no original: ',sum(features_df.duplicated()))\n",
    "print('número de pessoas únicas no original: ', features_df['person'].nunique())\n",
    "features_df.drop_duplicates(keep='first', inplace=True)\n",
    "print('total sem duplicados:      ',len(features_df))\n",
    "print('duplicados no final: ',sum(features_df.duplicated()))\n",
    "print('número de pessoas únicas sem dupicados: ', features_df['person'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uma função para remover outliers do dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(original_dataset, lower_threshold, upper_threshold, column_names=[]):\n",
    "    ''' Remove outliers from a dataframe\n",
    "        Everything above or below these percentiles will be cut off\n",
    "    '''\n",
    "    # TODO: add treatment for not numerical columns\n",
    "    dataset = original_dataset.copy()\n",
    "    \n",
    "    if column_names:\n",
    "        for column in column_names:\n",
    "            removed_outliers = remove(dataset[column], lower_threshold, upper_threshold)\n",
    "            # save the indexes of rows that must be removed\n",
    "            indexes_for_removal = dataset[column][~removed_outliers].index\n",
    "            # in fact remove outliers from this column \n",
    "            #print(indexes_for_removal)\n",
    "            dataset.drop(indexes_for_removal, inplace=True)\n",
    "            #print(f'removed {len(indexes_for_removal)} outliers for column {column}')\n",
    "            #print(f'remaining itens in dataset: {len(dataset)}')\n",
    "        return dataset\n",
    "            \n",
    "    else:\n",
    "        column_names = list(dataset.columns)\n",
    "        for column in column_names:\n",
    "            removed_outliers = remove(dataset[column], lower_threshold, upper_threshold)\n",
    "            # save the indexes of rows that must be removed\n",
    "            indexes_for_removal = dataset[column][~removed_outliers].index\n",
    "            # in fact remove outliers from this column \n",
    "            dataset.drop(indexes_for_removal, inplace=True)\n",
    "        return dataset\n",
    "    \n",
    "def remove(series, lower_threshold, upper_threshold):\n",
    "    ''' Remove outliers from a single pandas Series '''\n",
    "    # create a boolean mask where False values are the outliers\n",
    "    removed_outliers = series.between(series.quantile(lower_threshold),\n",
    "                                      series.quantile(upper_threshold))\n",
    "    return removed_outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testando com um classificador *random forests*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LabelEnconder, caso necessário"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# caso precisarmos de um label enconder\n",
    "#le = LabelEncoder()\n",
    "#ecg_id_features['person'] = le.fit_transform(ecg_id_features['person'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remoção de outliers do rr_interval (feature mais caótica)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original features had 114329 rows\n",
      "Features without outliers has 93378 rows\n"
     ]
    }
   ],
   "source": [
    "lower_threshold = 0.005 # 0,5%\n",
    "upper_threshold = 0.995 # 0,5%\n",
    "\n",
    "#lower_threshold = 0.01 # 1%\n",
    "#upper_threshold = 0.99 # 1%\n",
    "\n",
    "column_names = list(features_df.columns)\n",
    "person_index = column_names.index('person')\n",
    "column_names.pop(person_index)\n",
    "print(f'Original features had {len(features_df)} rows')\n",
    "\n",
    "cleaned_features_df = remove_outliers(features_df,lower_threshold, upper_threshold, column_names=column_names)\n",
    "\n",
    "print(f'Features without outliers has {len(cleaned_features_df)} rows')\n",
    "\n",
    "#cleaned_features_df.to_csv('driver_db_cleaned_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove os rótulos\n",
    "X = cleaned_features_df.drop(['person'], axis=1)\n",
    "# Preenche os dados faltosos com a média da respectiva coluna\n",
    "X = X.apply(lambda x: x.fillna(x.mean()))\n",
    "# O objeto da predição é 'y' (os rótulos das classes)\n",
    "y = cleaned_features_df['person']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed:    7.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed:    6.5s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  40 out of  40 | elapsed:   15.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed:   25.5s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  80 out of  80 | elapsed:   27.8s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:   35.6s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 120 out of 120 | elapsed:   41.6s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 140 out of 140 | elapsed:   56.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 160 out of 160 | elapsed:   57.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 180 out of 180 | elapsed:  1.2min finished\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_X = scaler.fit_transform(X_train)\n",
    "test_X = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "size_of_chunk = 10\n",
    "list_of_training_x = np.array_split(X_train, 10)\n",
    "list_of_training_y = np.array_split(y_train, 10)\n",
    "\n",
    "\n",
    "estimators=20\n",
    "rf = RandomForestClassifier(n_estimators=estimators, max_depth=100, min_samples_leaf=3, min_samples_split=10, verbose=True)\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    rf.fit(list_of_training_x[i], list_of_training_y[i])\n",
    "    \n",
    "    rf.set_params(n_estimators=estimators)\n",
    "    estimators+=20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separar os dados em porções de treino e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove os rótulos\n",
    "X = cleaned_features_df.drop(['person'], axis=1)\n",
    "# Preenche os dados faltosos com a média da respectiva coluna\n",
    "X = X.apply(lambda x: x.fillna(x.mean()))\n",
    "# O objeto da predição é 'y' (os rótulos das classes)\n",
    "y = cleaned_features_df['person']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Realizar Standardization nos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-13-b78a8e815016>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-13-b78a8e815016>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    for X, y in\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "chunks = np.array_split(cleaned_features_df, 3)\n",
    "print(type(chunks[0]))\n",
    "for chunk in chunks:\n",
    "    for X, y in "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dá uma olhada nos dados aṕos filtragem e Standardzation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X.plot(figsize=(12,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alimenta o classificador com os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = np.array_split(df, 3)\n",
    "print(type(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os parametros passados ao classificador são os utilizados\n",
    "# previamente, obtidos pelo GridSearch\n",
    "rf = RandomForestClassifier(n_jobs=4, n_estimators=200, max_depth=100, min_samples_leaf=3, min_samples_split=10, verbose=True)\n",
    "#print(y_train)\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_output = classification_report(y_test, predictions)\n",
    "print(classification_output)\n",
    "##f = open(\"demofile.txt\", \"w\")\n",
    "#f.write(classification_output)\n",
    "#f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verificando a importância das features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_names = list(features_df.columns)\n",
    "importances = [(feature, round(importance, 2)) for feature, importance in zip(columns_names, rf.feature_importances_)]\n",
    "sorted_importances = sorted(importances, key=lambda x:x[1], reverse=True) \n",
    "\n",
    "for pair in sorted_importances:\n",
    "    print('Feature: {:20} Importance: {}'.format(*pair))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values = range(len(rf.feature_importances_))\n",
    "plt.figure(figsize=(16,7))\n",
    "plt.ylabel('Importance')\n",
    "plt.xlabel('Feature')\n",
    "plt.title('Feature Importance')\n",
    "plt.xticks(x_values, columns_names, rotation='vertical')\n",
    "plt.bar(x_values, rf.feature_importances_, orientation = 'vertical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizando os motoristas com as menores acurácias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_driver_info(driver_info):\n",
    "    # driver_info is a dataset\n",
    "\n",
    "    # Cria 2 subplots\n",
    "    fig, axs = plt.subplots(1,2, figsize=(21,8))\n",
    "\n",
    "    # Visualização Geral\n",
    "    driver_info.plot(ax=axs[0])\n",
    "    plt.title(\"Drive 05\",fontsize=20)\n",
    "\n",
    "    # Boxplot com possíveis outliers\n",
    "    driver_info.plot(kind='box', ax=axs[1])\n",
    "    plt.title(\"'Pontos fora da curva' saltando para fora das caixas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# para o driver 05\n",
    "plot_driver_info(cleaned_features_df[cleaned_features_df['person'] == 'drive05'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# para o driver 01 - ALTA PRECISÃO\n",
    "#plot_driver_info(ecg_id_features[ecg_id_features['person'] == 'drive02'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_driver_info(features_df[features_df['person'] == 'drive07'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_driver_info(cleaned_features_df[cleaned_features_df['person'] == 'drive09'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_driver_info(cleaned_features_df[cleaned_features_df['person'] == 'drive11'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Remove os rótulos\n",
    "X = features_df.drop(['person'], axis=1)\n",
    "Preenche os dados faltosos com a média da respectiva coluna\n",
    "X = X.apply(lambda x: x.fillna(x.mean()))\n",
    "O objeto da predição é 'y' (os rótulos das classes)\n",
    "y = features_df['person']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(rf, X, y, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "np.split(lista, 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
